# 修改清单 - 已完成 ✅

## 1. ✅ SVO 标签的对齐问题 (Alignment Issue)
**状态：已修复**

**问题描述：**
- 原代码直接截断/填充 Target 的 SVO 标签给 Source 使用
- 导致删除主语后，"学习"（谓语）被错误标记为主语

**解决方案：**
- 在 `preprocess.py` 中实现了 `align_tokens_with_difflib()` 函数
- 使用 Python 内置的 `difflib.SequenceMatcher` 进行序列对齐
- 返回对齐后的 GEC 标签和 target_indices 映射
- 根据映射正确分配 SVO 标签

**关键代码：**
```python
def align_tokens_with_difflib(source_tokens, target_tokens):
    matcher = difflib.SequenceMatcher(None, source_tokens, target_tokens)
    # 使用 opcodes 进行精确对齐
    # 返回 (gec_labels, target_indices)
```

---

## 2. ✅ GECToR 标签生成的改进
**状态：已完成**

**问题描述：**
- 原代码只做简单的按位比较 `src[i] == tgt[i]`
- 无法处理插入/删除导致的位移

**解决方案：**
- 使用 `difflib.SequenceMatcher` 的 `get_opcodes()` 方法
- 支持四种操作：equal (KEEP), replace (REPLACE), delete (DELETE), insert (APPEND)
- 实现了完整的 KEEP/DELETE/APPEND/REPLACE 标签生成

**改进点：**
- equal → KEEP
- replace → REPLACE_{char} 或处理长度不匹配的情况
- delete → DELETE
- insert → 在前一个 token 标记 APPEND_{chars}

---

## 3. ✅ 数据增强策略的优化 - 精准删除
**状态：已实现**

**问题描述：**
- 原 `_add_word_order_error` 只能交换"X的Y"结构
- 无法针对性训练主谓宾缺失检测

**解决方案：**
- 用 `_add_component_deletion_error()` 替换词序错误函数
- 支持两种模式：
  1. **LTP模式**（可选）：识别主语(SBV)和谓语(HED)，精准删除
  2. **降级模式**：使用 jieba 分词后随机删除词
- 在 `ErrorGenerator.__init__()` 中添加 `use_ltp` 参数

**关键代码：**
```python
class ErrorGenerator:
    def __init__(self, use_ltp=False):
        if use_ltp and LTP_AVAILABLE:
            self.ltp = LTP("LTP/base")
    
    def _add_component_deletion_error(self, original_text, tokens):
        # 使用 LTP 识别主谓宾并删除
```

---

## 4. ✅ 新增虚词/介词删除增强
**状态：已实现**

**功能描述：**
- 实现 `_add_preposition_deletion()` 函数
- 优先删除虚词候选集中的字符：['的','地','得','了','是','在','对','于','和','与','及','或','而','以','为','把','被','将','给']
- 如果候选集为空，不做修改（避免误删实词）

**代码位置：**
`src/utils/augmentation.py` - `ErrorGenerator._add_preposition_deletion()`

---

## 5. ✅ 改进插入字符错误 - 增加重复字
**状态：已完成**

**改进内容：**
- 修改 `_add_insertion_error()` 函数
- 50% 概率插入重复字（选择当前位置的字符重复）
- 50% 概率插入常用虚词
- 返回的 error_info 增加 'subtype' 字段标识类型

**关键逻辑：**
```python
if random.random() < 0.5:
    # 插入重复字
    repeat_char = tokens[ins_idx]
    new_tokens = tokens[:ins_idx] + [repeat_char] + tokens[ins_idx:]
else:
    # 插入常用虚词
    ins_char = random.choice(common_chars)
```

---

## 6. ✅ 实现句级错误检测任务
**状态：已完成**

**实现内容：**

### 6.1 数据层面 (`preprocess.py`)
- 在生成的 sample 中添加 `sent_has_error` 字段（值为1表示有错误）

### 6.2 模型层面 (`modeling.py`)
- 在 `GECModelWithMTL` 中添加句级分类头：
  ```python
  self.sent_error_classifier = nn.Linear(config.hidden_size, 2)
  ```
- 使用 `pooled_output`（[CLS] token）进行句子级别分类
- forward() 返回三个 logits: `(gec_logits, svo_logits, sent_logits)`

### 6.3 数据集层面 (`dataset.py`)
- 在 `__getitem__()` 中读取 `sent_has_error` 字段
- 返回 `sent_label` tensor

### 6.4 损失函数层面 (`loss.py`)
- `MultiTaskLoss` 扩展为三个任务：
  - L_GEC (Focal Loss)
  - L_SVO (CrossEntropy)  
  - L_SENT (CrossEntropy) ← 新增
- 添加 `mtl_lambda_sent` 参数（默认0.3）
- 总损失：`L_total = L_GEC + λ1*L_SVO + λ2*L_SENT`

---

## 总结

所有6项修改任务已全部完成：

1. ✅ 使用 difflib 修复 SVO 标签对齐问题
2. ✅ 使用 difflib 改进 GECToR 标签生成
3. ✅ 实现精准成分删除（替换词序错误）
4. ✅ 新增虚词/介词删除增强
5. ✅ 改进插入错误（增加重复字逻辑）
6. ✅ 完整实现句级错误检测任务

### 关键改进点：
- **使用内置库 difflib**：避免手写复杂的对齐算法
- **LTP 集成**：支持精准的句法成分删除（可选）
- **多任务学习增强**：从2任务扩展到3任务
- **数据增强优化**：针对政府公文特点定制错误类型

### 注意事项：
- LTP 在 `ErrorGenerator` 中默认不启用（`use_ltp=False`），避免性能问题
- 如需启用精准删除，在实例化时传入 `use_ltp=True`
- 所有修改向后兼容，不影响现有代码运行

### 需要同步更新的其他模块：
如果你有 `trainer.py` 或其他训练脚本，需要确保：
1. 模型 forward 时传入 `sent_labels` 参数
2. 损失函数接收 `sent_logits` 和 `sent_labels`
3. 返回的损失元组现在是4个值：`(total_loss, gec_loss, svo_loss, sent_loss)`
