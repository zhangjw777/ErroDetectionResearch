1. 致命漏洞：SVO 标签的对齐问题 (Alignment Issue)
在 `preprocess.py` 的 `generate_training_data` 函数中，现有处理逻辑如下：

1. 对 正确句子 (Target) 提取 SVO 标签。
2. 生成 错误句子 (Source)（例如删除了主语）。
3. 直接截断/填充 Target 的 SVO 标签给 Source 使用。

❌ 这是一个严重的错误。

举例说明后果：
- 正确句子 (Target)：`我们(B-SUB) 学习(B-PRED) 文件(B-OBJ)。`
  - `target_tokens`: `['我', '们', '学', '习', '文', '件']`
  - `target_svo`: `['B-SUB', 'I-SUB', 'B-PRED', 'I-PRED', 'B-OBJ', 'I-OBJ']`
- 错误生成 (Source)：删除了主语“我们”。
  - `source_tokens`: `['学', '习', '文', '件']` (长度 4)
- 现有代码逻辑：
  - `source_svo_labels = target_svo_labels[:4]`
  - `source_svo_labels` 变成了: `['B-SUB', 'I-SUB', 'B-PRED', 'I-PRED']`
- 模型学到了什么？
  - 模型看到字符“学”，标签是 `B-SUB`（主语）。
  - 模型看到字符“习”，标签是 `I-SUB`。
  - 结果：实际上在教模型把“学习”（谓语）识别为主语。这会彻底搞乱模型的句法认知。
✅ 修正方案：基于 Edit-Distance 的标签映射
需要计算 Source 和 Target 之间的 Diff (编辑距离)，然后根据 Diff 结果来映射标签。
- 如果 Target 中的字在 Source 中被 删除 了，那么对应的 SVO 标签也应该丢弃。
- 如果 Source 中 插入 了新字，该新字的 SVO 标签应设为 `O`。
- 如果发生了 替换，继承原标签。

------
2. GECToR 标签生成的缺陷
现有代码中注释写道：简化版本：使用简单的diff策略。
对于硕士论文，这个简化是不够的，尤其是文档中提到了 APPEND_{char} 这种高级标签。
- 当前逻辑：只做 `src[i] == tgt[i]` 的按位比较。这处理不了 错序 和 插入/删除导致的位移。
- 后果：如果在句首插入了一个字，后面所有的字都会对不齐，导致后面所有字都被标记为 `REPLACE`，而不是 `KEEP`。这会导致模型学习非常低效。
- 建议：必须引入标准的 GECToR 预处理脚本（通常基于 Levenshtein Distance 动态规划算法）来生成最小编辑操作序列。实现上可以这样（思路级别）：
    1. 用编辑距离 / LCS 对齐 source_tokens 与 target_tokens；

    2. 对齐路径中：
        - source[i] == target[j] → KEEP；
        - source[i] != target[j] → REPLACE_{target[j]}；
        - source 多出的 → DELETE；
        - target 多出的 → 在前一个 source token 上打 APPEND_{target[j]}（或统一成 APPEND_MASK）。

    你可以从简单版开始：

    所有 insert（target 多出的 char）都变成 APPEND_MASK（不区分具体字符），减少标签数；

    真正 REPLACE 时，保留几个高频混淆字的 REPLACE_{char}，其余也映射到 REPLACE_MASK

------

3. 数据增强策略的优化 (针对高召回率)
研究文档强调 “召回率要求特别高”，且关注 “成分缺失”。但在 `augmentation.py` 中：
- `AUG_DELETION_ERROR_RATE` 是随机删除字符。
- 问题：随机删除可能删除“的”、“了”这种无关紧要的字，而不是关键的主谓宾。
- 优化：结合 LTP 的结果进行精准删除。
  - 策略：在 `ErrorGenerator` 中引入 LTP 结果。
  - 操作：专门编写一个 `remove_subject` 或 `remove_predicate` 函数。如果 LTP 识别出 `SBV`，就以 50% 的概率把整个主语删掉。这种样本对现有 `L_SVO` 辅助任务是最强的训练信号。用这个新的删除成分来代替词序错误_add_word_order_error。
---
## 第二轮修复（用户反馈并已实现）

### 6. **修复 FocalLoss 的 ignore_index 致命bug**

**问题描述**：
- FocalLoss 在 `gather` 操作之前没有处理 `ignore_index=-100`，会直接尝试访问非法索引导致训练崩溃。

**修复内容**：
- 在 `loss.py` 中：先创建 mask (`mask = (targets != self.ignore_index)`)，并将 `ignore_index` 位置临时映射为合法标签（例如 0）用于 `gather`，随后通过 mask 清零这些位置的 loss；同时在 mean 归一化时使用 `(mask * label_mask).sum()`（若提供 `label_mask`）作为分母。

**影响文件**：`src/loss.py`

---

### 7. **修复 predictor 推理时的输出解包错误并暴露句级结果**

**问题描述**：
- `GECPredictor.predict()` 之前只解包两个输出，但模型返回三个（`gec_logits, svo_logits, sent_logits`），导致 ValueError。

**修复内容**：
- 修改为接收 `sent_logits`，计算句级错误概率并在返回结果中添加 `sent_has_error` 与 `sent_error_prob`。方便后续统计和可视化。

**影响文件**：`src/predictor.py`

---

### 8. **修复 create_dataloaders 中 max_length 未生效问题**

**问题描述**：
- `GECDataset` 支持 `max_length`，但 `create_dataloaders()` 未将 `cfg.MAX_SEQ_LENGTH` 传递进去，导致配置变更无效。

**修复内容**：
- 在 `dataset.create_dataloaders()` 中增加 `max_length` 参数，并在 `trainer.py` 调用时传入 `cfg.MAX_SEQ_LENGTH`。

**影响文件**：`src/dataset.py`, `src/trainer.py`

---

### 9. **实现 APPEND_MASK / REPLACE_MASK 的标签压缩策略（轻量化）**

**问题描述**：
- 研究文档要求仅为高频虚词保留专门标签，其余统一为 MASK，以减少标签空间。但预处理代码最初直接使用完整插入/替换内容。

**修复内容**：
- 在 `config.py` 添加 `HIGH_FREQ_FUNCTION_WORDS` 与 `ENABLE_LABEL_COMPRESSION` 开关；
- 在 `preprocess.align_tokens_with_difflib()` 中：对 REPLACE/APPEND 操作应用压缩策略 —— 非高频词用 `*_MASK` 标签；保留混淆词集合中的字符不做压缩。

**影响文件**：`src/config.py`, `src/preprocess.py`

---

## 验证与说明

- 已对修改后的文件运行语法检查（`get_errors`），所有修改的文件均未报告语法错误。
- 特别说明：修复 FocalLoss 的 ignore_index 问题是训练能否启动的关键，已优先完成。

## 后续建议

- 优先实现并验证 SVO 标签基于编辑距离的映射（当前已使用 difflib 的 opcode 逻辑，但建议引入更严格的 Levenshtein 最小编辑序列以保证 GEC 标签的质量）。
- 在数据增强模块中引入基于句法的删除/插入操作（例如优先删除主语/谓语），以提高召回率。
- 添加端到端单元测试覆盖：数据加载 -> 标签对齐 -> 模型前向 -> 损失计算，确保这些关键链路不会回归。

4. 新增_add_preposition_deletion「倾向删虚词 / 介词 / 助词 / 连词」，比如：遍历 tokens，只把属于候选集合 ['的','地','得','了','是','在','对','于','和','与','及'] 的索引加入候选集；如果候选集为空此次就不做修改。
---
5. 插入字符错误_add_insertion_error除了加入常用字还需要有概率的在句子随机某个位置加入重复字 
---
6. 辅助任务1：句级错误检测还没有实现，需要在sample中增加一个sent_has_error字段，表示该句子是否有错误。其他训练/推理的模块也要同步修改。

