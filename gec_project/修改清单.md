1. 致命漏洞：SVO 标签的对齐问题 (Alignment Issue)
在 `preprocess.py` 的 `generate_training_data` 函数中，现有处理逻辑如下：

1. 对 正确句子 (Target) 提取 SVO 标签。
2. 生成 错误句子 (Source)（例如删除了主语）。
3. 直接截断/填充 Target 的 SVO 标签给 Source 使用。

❌ 这是一个严重的错误。

举例说明后果：
- 正确句子 (Target)：`我们(B-SUB) 学习(B-PRED) 文件(B-OBJ)。`
  - `target_tokens`: `['我', '们', '学', '习', '文', '件']`
  - `target_svo`: `['B-SUB', 'I-SUB', 'B-PRED', 'I-PRED', 'B-OBJ', 'I-OBJ']`
- 错误生成 (Source)：删除了主语“我们”。
  - `source_tokens`: `['学', '习', '文', '件']` (长度 4)
- 现有代码逻辑：
  - `source_svo_labels = target_svo_labels[:4]`
  - `source_svo_labels` 变成了: `['B-SUB', 'I-SUB', 'B-PRED', 'I-PRED']`
- 模型学到了什么？
  - 模型看到字符“学”，标签是 `B-SUB`（主语）。
  - 模型看到字符“习”，标签是 `I-SUB`。
  - 结果：实际上在教模型把“学习”（谓语）识别为主语。这会彻底搞乱模型的句法认知。
✅ 修正方案：基于 Edit-Distance 的标签映射
需要计算 Source 和 Target 之间的 Diff (编辑距离)，然后根据 Diff 结果来映射标签。
- 如果 Target 中的字在 Source 中被 删除 了，那么对应的 SVO 标签也应该丢弃。
- 如果 Source 中 插入 了新字，该新字的 SVO 标签应设为 `O`。
- 如果发生了 替换，继承原标签。

------
2. GECToR 标签生成的缺陷
现有代码中注释写道：简化版本：使用简单的diff策略。
对于硕士论文，这个简化是不够的，尤其是文档中提到了 APPEND_{char} 这种高级标签。
- 当前逻辑：只做 `src[i] == tgt[i]` 的按位比较。这处理不了 错序 和 插入/删除导致的位移。
- 后果：如果在句首插入了一个字，后面所有的字都会对不齐，导致后面所有字都被标记为 `REPLACE`，而不是 `KEEP`。这会导致模型学习非常低效。
- 建议：必须引入标准的 GECToR 预处理脚本（通常基于 Levenshtein Distance 动态规划算法）来生成最小编辑操作序列。实现上可以这样（思路级别）：
    1. 用编辑距离 / LCS 对齐 source_tokens 与 target_tokens；

    2. 对齐路径中：
        - source[i] == target[j] → KEEP；
        - source[i] != target[j] → REPLACE_{target[j]}；
        - source 多出的 → DELETE；
        - target 多出的 → 在前一个 source token 上打 APPEND_{target[j]}（或统一成 APPEND_MASK）。

    你可以从简单版开始：

    所有 insert（target 多出的 char）都变成 APPEND_MASK（不区分具体字符），减少标签数；

    真正 REPLACE 时，保留几个高频混淆字的 REPLACE_{char}，其余也映射到 REPLACE_MASK

------

3. 数据增强策略的优化 (针对高召回率)
研究文档强调 “召回率要求特别高”，且关注 “成分缺失”。但在 `augmentation.py` 中：
- `AUG_DELETION_ERROR_RATE` 是随机删除字符。
- 问题：随机删除可能删除“的”、“了”这种无关紧要的字，而不是关键的主谓宾。
- 优化：结合 LTP 的结果进行精准删除。
  - 策略：在 `ErrorGenerator` 中引入 LTP 结果。
  - 操作：专门编写一个 `remove_subject` 或 `remove_predicate` 函数。如果 LTP 识别出 `SBV`，就以 50% 的概率把整个主语删掉。这种样本对现有 `L_SVO` 辅助任务是最强的训练信号。用这个新的删除成分来代替词序错误_add_word_order_error。
---
4. 新增_add_preposition_deletion「倾向删虚词 / 介词 / 助词 / 连词」，比如：遍历 tokens，只把属于候选集合 ['的','地','得','了','是','在','对','于','和','与','及'] 的索引加入候选集；如果候选集为空此次就不做修改。
---
5. 插入字符错误_add_insertion_error除了加入常用字还需要有概率的在句子随机某个位置加入重复字 
---
6. 辅助任务1：句级错误检测还没有实现，需要在sample中增加一个sent_has_error字段，表示该句子是否有错误。其他训练/推理的模块也要同步修改。

