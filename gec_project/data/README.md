# GEC项目数据说明

## 数据目录结构

```
data/
├── raw/              # 原始公文语料
├── clean/            # 清洗后的正确句子
├── synthetic/        # 生成的训练数据
└── vocab/            # 词表和标签映射
```

## 原始数据格式

原始公文数据应为JSONL格式，每行一个JSON对象：

```json
{
  "subTitle": "",
  "dataTime": "2024-12-24",
  "contentText": "会议强调，要认真学习贯彻习近平总书记重要讲话精神...",
  "publishSource": "大众日报",
  "title": "市委经济工作会议召开",
  "introTitle": "..."
}
```

将JSONL文件放入 `raw/` 目录即可。

## 数据预处理流程

1. **清洗原始数据**
   ```bash
   python src/preprocess.py
   ```
   
   此步骤会：
   - 读取 `raw/` 目录下的所有JSONL文件
   - 提取 `contentText` 字段
   - **清理HTML标签**（`<p>`, `<br>`, `<div>` 等）
   - **清理HTML实体**（`&nbsp;`, `&lt;` 等）
   - 切分句子
   - 过滤不合规的句子（太短/太长）
   - 去重
   - 保存到 `clean/clean_sentences.txt`

2. **生成训练数据**
   
   预处理脚本会自动：
   - 为每个正确句子生成2-3个错误样本
   - 使用LTP提取SVO标签
   - 生成GECToR编辑标签
   - 划分训练集（90%）和验证集（10%）
   - 保存到 `synthetic/train.json` 和 `synthetic/dev.json`

3. **构建词表**
   
   自动生成：
   - `vocab/label_map.txt`: GEC标签映射
   - `vocab/svo_labels.txt`: SVO标签（固定7个）

## 训练数据格式

训练数据（train.json / dev.json）格式：

```json
[
  {
    "uid": "sent_0_sample_0",
    "text": "通过这次活动，使我们认识到了错误。",
    "tokens": ["通", "过", "这", "次", ...],
    "gec_labels": ["$KEEP", "$KEEP", ..., "$DELETE", ...],
    "svo_labels": ["O", "O", ..., "B-SUB", "I-SUB", ...]
  },
  ...
]
```

## 数据增强策略

针对公文领域的错误类型：

1. **介词滥用** (15%概率)
   - 在句首添加多余的"通过"、"经过"等

2. **混淆词错误** (20%概率)
   - 权利 ↔ 权力
   - 制定 ↔ 制订
   - 启用 ↔ 起用

3. **词序错误** (10%概率)
   - 识别"定语+的+中心语"结构并交换

4. **删除错误** (10%概率)
   - 随机删除非标点字符

5. **插入错误** (10%概率)
   - 随机插入常见虚词（的、了、和等）

## HTML标签清理说明

原始公文数据中可能包含HTML标签和实体，预处理会自动清理：

- **块级标签**：`<p>`, `<div>`, `<br>`, `<h1-6>`, `<li>`, `<table>` 等会被转换为空格
- **行内标签**：`<span>`, `<strong>`, `<em>` 等会被直接删除
- **HTML实体**：`&nbsp;`, `&lt;`, `&gt;`, `&#数字;` 等会被转换为空格
- **多余空白**：自动合并连续的空格、换行符等

示例：
```
输入: <p>会议强调，要认真学习</p><p>确保工作落实</p>
输出: 会议强调，要认真学习 确保工作落实
```

## 注意事项

- 确保原始数据的质量（公文应该是正确的）
- 数据预处理需要安装LTP：`pip install ltp`
- 首次运行LTP会自动下载模型文件（约400MB）
- 建议先用小规模数据测试（如1000句）
- 已自动处理HTML标签，无需手动预处理
